# üß† Mini Asistente de Q&A

Aplicaci√≥n web que permite subir documentos PDF/TXT, realizar b√∫squedas dentro de ellos y hacer preguntas en lenguaje natural, obteniendo respuestas breves con citas de los documentos.

---

## üöÄ Tecnolog√≠as Utilizadas

- **Backend**: FastAPI (Python)
- **Frontend**: React + TypeScript
- **Motor de b√∫squeda**: TF-IDF con Scikit-learn
- **LLM (modelo de lenguaje)**: OpenAI GPT-3.5-Turbo
- **Contenedores**: Docker + Docker Compose
- **Otros**: SweetAlert2, Lucide React, Tailwind CSS

---

## ‚úÖ Funcionalidades Implementadas

### üìÇ Subida de archivos

- Se pueden subir entre 3 y 10 archivos `.pdf` o `.txt`.
- Se valida el tipo de archivo y el tama√±o m√°ximo permitido (5 MB).
- Los archivos subidos se almacenan y pueden visualizarse o eliminarse desde la interfaz.

### üîç B√∫squeda de contenido (`/search`)

- Endpoint: `GET /search?q=...`
- Realiza b√∫squeda de los fragmentos m√°s relevantes mediante TF-IDF.
- Devuelve:
  - Fragmento del texto
  - Nombre del documento
  - Puntaje de relevancia

### ‚ùì Preguntas en lenguaje natural (`/ask`)

- Endpoint: `POST /ask`
- Recibe una pregunta como JSON (`{ "question": "..." }`).
- Responde en 3‚Äì4 l√≠neas utilizando un modelo de lenguaje (LLM).
- El contexto se construye con los fragmentos m√°s relevantes mediante TF-IDF.
- Las respuestas incluyen citas clicables hacia los documentos fuente.
- Se utiliza **GPT-3.5-Turbo** de OpenAI para generar la respuesta final.

### üì¶ Ingesta de documentos (`/ingest`)

- Endpoint: `POST /ingest`
- Procesa e indexa m√∫ltiples archivos recibidos en memoria.
- Divide en fragmentos para indexar con TF-IDF.

---

## üì∏ Capturas de Pantalla

![Demo QA Mini Asistente](./demo_qa_mini_asistente.gif)

## ‚öôÔ∏è Instrucciones de ejecuci√≥n

1. Clona el repositorio:

```bash
git clone https://github.com/CspO6/qa-mini-asistente
cd qa-mini-asistente
```

2. Configura las variables de entorno:

Copia el archivo `.env.example` y ren√≥mbralo como `.env` dentro de la carpeta `backend/`. Luego reemplaza el valor de la variable con tu clave de OpenAI:
```bash
cp .env.example .env
```
OPENAI_API_KEY=sk-...  # Reemplaza con tu propia clave de OpenAI

‚ö†Ô∏è Nota importante:
Este archivo no est√° incluido en el repositorio por motivos de seguridad. Sin este archivo, no podr√°s realizar preguntas a los documentos utilizando la API de OpenAI.

3. Levanta los servicios:

docker-compose up --build


4. Accede a la app:

Frontend: http://localhost:3000
Backend: http://localhost:8000

üß† Decisiones t√©cnicas y supuestos

- Decid√≠ utilizar **TF-IDF** para hacer la b√∫squeda de fragmentos relevantes porque es una soluci√≥n sencilla, r√°pida y no requiere grandes recursos. Esto permite encontrar coincidencias en los documentos sin tener que usar bases de datos complejas ni t√©cnicas de embeddings.
- Para responder preguntas, decid√≠ usar GPT-3.5-Turbo de OpenAI como modelo de lenguaje. Lo eleg√≠ porque permite generar respuestas m√°s naturales, claras y espec√≠ficas, en lugar de limitarse a devolver fragmentos de texto. Primero, el sistema hace una b√∫squeda sem√°ntica con TF-IDF, identifica los fragmentos m√°s relevantes, y luego se los pasa como contexto al modelo de OpenAI, que genera una respuesta usando solo esa informaci√≥n. Esta integraci√≥n permite que el usuario obtenga una respuesta breve y √∫til, sin tener que leer todo el documento.
- Todos los archivos que se suben se almacenan localmente dentro de la carpeta `uploaded_files/`. No se usan servicios externos de almacenamiento por ahora para mantenerlo simple y local.
- No se utiliza un √≠ndice persistente. Toda la informaci√≥n se carga y procesa cada vez que se hace una pregunta, ya que el volumen de documentos en esta prueba t√©cnica es bajo y no afecta el rendimiento.
- Se agreg√≥ validaci√≥n tanto en el frontend como en el backend para limitar el tama√±o y tipo de archivo permitido (solo `.pdf` y `.txt`, hasta 5MB), para evitar errores y mantener la app m√°s segura y controlada.

‚è±Ô∏è Tiempo invertido

12.5 horas en total:
- Backend (FastAPI): 3 horas
- Frontend (React + TypeScript): 3 horas
- Integraci√≥n con OpenAI (implementaci√≥n, ajustes de contexto y citaciones): 3.5 horas
- Validaciones, estilos y mejoras visuales: 1.5 horas
- Docker y documentaci√≥n: 0.5 horas
- Pruebas finales y ajustes: 1 hora

üß™ Bonus Implementados

- Validaciones robustas de archivos (.txt, .pdf, 5MB)
- UI clara con estados de carga, error y √©xito
- Citaciones clicables con enlaces al archivo
- √çndice dividido por fragmentos
- C√≥digo limpio y modular
- Pruebas de backend

üêç Backend (FastAPI + Pytest)
Se utiliza pytest para ejecutar las pruebas del backend.

üì¶ Instalar dependencias
Hay que asegurarse de tener un entorno virtual y las dependencias instaladas:

```bash
cd backend
python -m venv env
env\Scripts\activate
pip install -r requirements.txt
```

‚ñ∂Ô∏è Ejecutar pruebas

```bash
pytest
```